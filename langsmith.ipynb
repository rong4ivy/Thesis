{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for CustomPromptTemplate\ninput_variables\n  field required (type=value_error.missing)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 122\u001b[0m\n\u001b[1;32m    119\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([tool\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtools])\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemplate\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 122\u001b[0m agent_prompt \u001b[38;5;241m=\u001b[39m CustomPromptTemplate(\n\u001b[1;32m    123\u001b[0m     template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mAnswer the following question as best you can:\u001b[39m\n\u001b[1;32m    124\u001b[0m \n\u001b[1;32m    125\u001b[0m \u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{input}\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \n\u001b[1;32m    127\u001b[0m \u001b[38;5;124mYou have access to the following tools:\u001b[39m\n\u001b[1;32m    128\u001b[0m \n\u001b[1;32m    129\u001b[0m \u001b[38;5;132;01m{tools}\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \n\u001b[1;32m    131\u001b[0m \u001b[38;5;124mUse the following format:\u001b[39m\n\u001b[1;32m    132\u001b[0m \n\u001b[1;32m    133\u001b[0m \u001b[38;5;124mThought: you should always think about what to do\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124mAction: the action to take, should be one of [\u001b[39m\u001b[38;5;132;01m{tool_names}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124mAction Input: the input to the action\u001b[39m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124mObservation: the result of the action\u001b[39m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124m... (this Thought/Action/Action Input/Observation can repeat N times)\u001b[39m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124mThought: I now know the final answer\u001b[39m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124mFinal Answer: the final answer to the original input question\u001b[39m\n\u001b[1;32m    140\u001b[0m \n\u001b[1;32m    141\u001b[0m \u001b[38;5;124mBegin!\u001b[39m\n\u001b[1;32m    142\u001b[0m \n\u001b[1;32m    143\u001b[0m \u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{input}\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;132;01m{agent_scratchpad}\u001b[39;00m\u001b[38;5;124m\"\"\"\u001b[39m,\n\u001b[1;32m    145\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m    146\u001b[0m )\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# Define LLM chain\u001b[39;00m\n\u001b[1;32m    149\u001b[0m llm_chain \u001b[38;5;241m=\u001b[39m LLMChain(llm\u001b[38;5;241m=\u001b[39mllm, prompt\u001b[38;5;241m=\u001b[39magent_prompt)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/langchain/lib/python3.12/site-packages/langchain_core/load/serializable.py:113\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/langchain/lib/python3.12/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for CustomPromptTemplate\ninput_variables\n  field required (type=value_error.missing)"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks import LangChainTracer\n",
    "from langsmith import Client\n",
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "from typing import List, Union\n",
    "import clingo\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Set your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key\"  # Replace with your OpenAI API key\n",
    "# Set your Groq API key\n",
    "os.environ['GROQ_API_KEY'] = \""\n",
    "os.environ['DEEPSEEK_API_KEY'] = \""\n",
    "os.environ['LLAMA_API_KEY']= \"Q0\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"\"  # Update with your API key\n",
    "\n",
    " \n",
    "# Initialize LangSmith client\n",
    "client = Client()\n",
    "\n",
    "# Step 1: Convert natural language to atomic facts\n",
    "nl_to_facts_prompt = PromptTemplate(\n",
    "    input_variables=[\"nl_input\"],\n",
    "    template=\"Convert the following natural language into atomic facts:\\n{nl_input}\\n\\nAtomic facts:\"\n",
    ")\n",
    "\n",
    "nl_to_facts_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=nl_to_facts_prompt,\n",
    "    verbose=True,\n",
    "    tags=[\"nl_to_facts\"],\n",
    ")\n",
    "\n",
    "# Step 2: Add rules to form complete ASP program\n",
    "asp_rules_prompt = PromptTemplate(\n",
    "    input_variables=[\"atomic_facts\"],\n",
    "    template=\"Given these atomic facts:\\n{atomic_facts}\\n\\nAdd rules to form a complete ASP program:\"\n",
    ")\n",
    "\n",
    "asp_rules_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=asp_rules_prompt,\n",
    "    verbose=True,\n",
    "    tags=[\"asp_rules\"],\n",
    ")\n",
    "\n",
    "# Step 3: Debug ASP program\n",
    "debug_asp_prompt = PromptTemplate(\n",
    "    input_variables=[\"asp_program\"],\n",
    "    template=\"Debug the following ASP program and correct any syntax errors:\\n{asp_program}\\n\\nCorrected ASP program:\"\n",
    ")\n",
    "\n",
    "debug_asp_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=debug_asp_prompt,\n",
    "    verbose=True,\n",
    "    tags=[\"debug_asp\"],\n",
    ")\n",
    "\n",
    "# Step 4: Run ASP program\n",
    "def run_asp_program(asp_program: str) -> List[str]:\n",
    "    ctl = clingo.Control()\n",
    "    ctl.add(\"base\", [], asp_program)\n",
    "    ctl.ground([(\"base\", [])])\n",
    "    models = []\n",
    "    with ctl.solve(yield_=True) as handle:\n",
    "        for model in handle:\n",
    "            models.append(str(model))\n",
    "    return models\n",
    "\n",
    "# Step 5: Choose best answer\n",
    "choose_answer_prompt = PromptTemplate(\n",
    "    input_variables=[\"asp_results\", \"original_question\"],\n",
    "    template=\"Given these ASP results:\\n{asp_results}\\n\\nAnd the original question:\\n{original_question}\\n\\nChoose the best answer:\"\n",
    ")\n",
    "\n",
    "choose_answer_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=choose_answer_prompt,\n",
    "    verbose=True,\n",
    "    tags=[\"choose_answer\"],\n",
    ")\n",
    "\n",
    "# Define tools for the agent\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"NL to Facts\",\n",
    "        func=nl_to_facts_chain.run,\n",
    "        description=\"Convert natural language to atomic facts\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"ASP Rules\",\n",
    "        func=asp_rules_chain.run,\n",
    "        description=\"Add rules to form complete ASP program\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Debug ASP\",\n",
    "        func=debug_asp_chain.run,\n",
    "        description=\"Debug the ASP program and correct any syntax errors\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Run ASP\",\n",
    "        func=run_asp_program,\n",
    "        description=\"Run the ASP program and return results\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Choose Answer\",\n",
    "        func=choose_answer_chain.run,\n",
    "        description=\"Choose the best answer from ASP results\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Define agent prompt\n",
    "class CustomPromptTemplate(StringPromptTemplate):\n",
    "    template: str\n",
    "    tools: List[Tool]\n",
    "\n",
    "    def format(self, **kwargs) -> str:\n",
    "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
    "        thoughts = \"\"\n",
    "        for action, observation in intermediate_steps:\n",
    "            thoughts += action.log\n",
    "            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n",
    "        kwargs[\"agent_scratchpad\"] = thoughts\n",
    "        kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n",
    "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n",
    "        return self.template.format(**kwargs)\n",
    "\n",
    "agent_prompt = CustomPromptTemplate(\n",
    "    template=\"\"\"Answer the following question as best you can:\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\"\"\",\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "# Define LLM chain\n",
    "llm_chain = LLMChain(llm=llm, prompt=agent_prompt)\n",
    "\n",
    "# Define agent\n",
    "agent = LLMSingleActionAgent(\n",
    "    llm_chain=llm_chain,\n",
    "    output_parser=None,\n",
    "    stop=[\"\\nObservation:\"],\n",
    "    allowed_tools=[tool.name for tool in tools]\n",
    ")\n",
    "\n",
    "# Create agent executor\n",
    "agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# Function to process the JSON file and run the pipeline\n",
    "def process_json_file(json_file_path):\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    results = []\n",
    "    for item in data:\n",
    "        context = item.get('context', '')\n",
    "        question = item.get('question', '')\n",
    "        predicted_asp = item.get('predicted', '')\n",
    "\n",
    "        # Run the pipeline\n",
    "        with LangChainTracer(client=client) as tracer:\n",
    "            atomic_facts = nl_to_facts_chain.run(nl_input=question)\n",
    "            asp_program = asp_rules_chain.run(atomic_facts=atomic_facts)\n",
    "            corrected_asp = debug_asp_chain.run(asp_program=asp_program)\n",
    "            asp_results = run_asp_program(corrected_asp)\n",
    "            best_answer = choose_answer_chain.run(asp_results=asp_results, original_question=question)\n",
    "\n",
    "        results.append({\n",
    "            \"context\": context,\n",
    "            \"question\": question,\n",
    "            \"predicted\": predicted_asp,\n",
    "            \"actual_answer\": item.get('answer', ''),\n",
    "            \"atomic_facts\": atomic_facts,\n",
    "            \"asp_program\": asp_program,\n",
    "            \"corrected_asp\": corrected_asp,\n",
    "            \"asp_results\": asp_results,\n",
    "            \"best_answer\": best_answer\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Process the JSON file and save the results\n",
    "input_json_file = \"results_optimized.json\"\n",
    "output_json_file = \"results_with_pipeline_output.json\"\n",
    "results = process_json_file(input_json_file)\n",
    "\n",
    "with open(output_json_file, 'w') as file:\n",
    "    json.dump(results, file, indent=4)\n",
    "\n",
    "print(f\"Processing complete. Results saved to {output_json_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.agents import Tool\n",
    "import clingo\n",
    "from typing import List\n",
    "\n",
    "# Initialize OpenAI LLM\n",
    "llm = OpenAI(\n",
    "    model=\"deepseek-chat\",\n",
    "    api_base=\"https://api.deepseek.com\",\n",
    "    api_key=\"sk-a09073e75492461e90732d16b43821e2\",\n",
    "    model_type='chat',\n",
    "    max_tokens=4096,\n",
    "    temperature=0.1,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0.1,\n",
    "    presence_penalty=0.1,\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "# Step 1: Convert natural language to atomic facts\n",
    "nl_to_facts_prompt = PromptTemplate(\n",
    "    input_variables=[\"nl_input\"],\n",
    "    template=\"Convert the following natural language into atomic facts:\\n{nl_input}\\n\\nAtomic facts:\"\n",
    ")\n",
    "\n",
    "nl_to_facts_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=nl_to_facts_prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Step 2: Add rules to form complete ASP program\n",
    "asp_rules_prompt = PromptTemplate(\n",
    "    input_variables=[\"atomic_facts\"],\n",
    "    template=\"Given these atomic facts:\\n{atomic_facts}\\n\\nAdd rules to form a complete ASP program:\"\n",
    ")\n",
    "\n",
    "asp_rules_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=asp_rules_prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Step 3: Debug ASP program\n",
    "debug_asp_prompt = PromptTemplate(\n",
    "    input_variables=[\"asp_program\"],\n",
    "    template=\"Debug the following ASP program and correct any syntax errors:\\n{asp_program}\\n\\nCorrected ASP program:\"\n",
    ")\n",
    "\n",
    "debug_asp_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=debug_asp_prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Step 4: Run ASP program\n",
    "def run_asp_program(asp_program: str) -> List[str]:\n",
    "    ctl = clingo.Control()\n",
    "    ctl.add(\"base\", [], asp_program)\n",
    "    ctl.ground([(\"base\", [])])\n",
    "    results = []\n",
    "    with ctl.solve(yield_=True) as handle:\n",
    "        for model in handle:\n",
    "            results.append(str(model))\n",
    "    return results\n",
    "\n",
    "# Step 5: Choose best answer\n",
    "choose_answer_prompt = PromptTemplate(\n",
    "    input_variables=[\"asp_results\", \"original_question\"],\n",
    "    template=\"Given these ASP results:\\n{asp_results}\\n\\nAnd the original question:\\n{original_question}\\n\\nChoose the best answer:\"\n",
    ")\n",
    "\n",
    "choose_answer_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=choose_answer_prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Main pipeline function\n",
    "def pipeline(nl_input: str, original_question: str) -> str:\n",
    "    # Step 1: Convert natural language to atomic facts\n",
    "    atomic_facts = nl_to_facts_chain.run(nl_input=nl_input)\n",
    "\n",
    "    # Step 2: Add rules to form complete ASP program\n",
    "    asp_program = asp_rules_chain.run(atomic_facts=atomic_facts)\n",
    "\n",
    "    # Step 3: Debug ASP program\n",
    "    debugged_asp_program = debug_asp_chain.run(asp_program=asp_program)\n",
    "\n",
    "    # Step 4: Run ASP program\n",
    "    asp_results = run_asp_program(debugged_asp_program)\n",
    "\n",
    "    # Step 5: Choose the best answer\n",
    "    best_answer = choose_answer_chain.run(asp_results=asp_results, original_question=original_question)\n",
    "\n",
    "    return best_answer\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    nl_input = \"A person wants to know if they can go to the park.\"\n",
    "    original_question = \"Can I go to the park if it is raining?\"\n",
    "    \n",
    "    answer = pipeline(nl_input, original_question)\n",
    "    print(\"Best Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import Runnable, RunnablePassthrough, LangChainPredict, StrOutputParser, Memory\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "import clingo\n",
    "\n",
    "# Initialize OpenAI LLM\n",
    "llm = OpenAI(\n",
    "    model=\"deepseek-chat\",\n",
    "    api_base=\"https://api.deepseek.com\",\n",
    "    api_key=\"sk-a09073e75492461e90732d16b43821e2\",\n",
    "    model_type='chat',\n",
    "    max_tokens=4096,\n",
    "    temperature=0.1,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0.1,\n",
    "    presence_penalty=0.1,\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "# Step 1: Convert natural language to atomic facts\n",
    "nl_to_facts_prompt = PromptTemplate(\n",
    "    input_variables=[\"nl_input\"],\n",
    "    template=\"Convert the following natural language into atomic facts:\\n{nl_input}\\n\\nAtomic facts:\"\n",
    ")\n",
    "\n",
    "# Step 2: Add rules to form complete ASP program\n",
    "asp_rules_prompt = PromptTemplate(\n",
    "    input_variables=[\"atomic_facts\"],\n",
    "    template=\"Given these atomic facts:\\n{atomic_facts}\\n\\nAdd rules to form a complete ASP program:\"\n",
    ")\n",
    "\n",
    "# Step 3: Debug ASP program\n",
    "debug_asp_prompt = PromptTemplate(\n",
    "    input_variables=[\"asp_program\"],\n",
    "    template=\"Debug the following ASP program and correct any syntax errors:\\n{asp_program}\\n\\nCorrected ASP program:\"\n",
    ")\n",
    "\n",
    "# Step 4: Run ASP program\n",
    "def run_asp_program(asp_program: str) -> str:\n",
    "    ctl = clingo.Control()\n",
    "    ctl.add(\"base\", [], asp_program)\n",
    "    ctl.ground([(\"base\", [])])\n",
    "    results = []\n",
    "    with ctl.solve(yield_=True) as handle:\n",
    "        for model in handle:\n",
    "            results.append(str(model))\n",
    "    return \"\\n\".join(results)\n",
    "\n",
    "# Step 5: Choose best answer\n",
    "choose_answer_prompt = PromptTemplate(\n",
    "    input_variables=[\"asp_results\", \"original_question\"],\n",
    "    template=\"Given these ASP results:\\n{asp_results}\\n\\nAnd the original question:\\n{original_question}\\n\\nChoose the best answer:\"\n",
    ")\n",
    "\n",
    "# Define the pipeline using LCEL with memory\n",
    "memory = Memory()\n",
    "\n",
    "pipeline = (\n",
    "    RunnablePassthrough.assign(nl_input=\"A person wants to know if they can go to the park.\")  # Replace with dynamic input\n",
    "    | LangChainPredict(prompt=nl_to_facts_prompt, llm=llm)  # Convert to atomic facts\n",
    "    | memory.store(\"atomic_facts\")  # Store atomic facts in memory\n",
    "    | RunnablePassthrough.assign(atomic_facts=lambda x: x)  # Pass atomic facts to the next step\n",
    "    | LangChainPredict(prompt=asp_rules_prompt, llm=llm)  # Generate ASP program\n",
    "    | LangChainPredict(prompt=debug_asp_prompt, llm=llm)  # Debug ASP program\n",
    "    | RunnablePassthrough.assign(asp_program=lambda x: x)  # Pass debugged ASP program to the next step\n",
    "    | Runnable(lambda x: run_asp_program(x))  # Run ASP program\n",
    "    | memory.store(\"asp_results\")  # Store ASP results in memory\n",
    "    | LangChainPredict(prompt=choose_answer_prompt, llm=llm)  # Choose the best answer\n",
    ")\n",
    "\n",
    "# Execute the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    original_question = \"Can I go to the park if it is raining?\"\n",
    "    result = pipeline.invoke({\"nl_input\": \"A person wants to know if they can go to the park.\", \"original_question\": original_question})\n",
    "    print(\"Best Answer:\", result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dspy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
