{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.evaluation import load_evaluator, EvaluatorType\n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load  from environment variable \n",
    "\n",
    "# Define prompt\n",
    "Base_prompt = \"\"\"Please answer the question by choosing the answer(s) from the candidate answers, where DK means \"Don't Know\".\n",
    "Important: Pay special attention to the output format. The output should be 1-5 words. Do not include the reasoning process and explanation in the output answer.\n",
    "\"\"\"\n",
    "CoT_prompt = \"\"\"\n",
    "You are an expert in spatial reasoning tasks. Always think step by step to answer the question.\n",
    "1. Carefully read the context and question, identify the key objects, their attributes and relative relations mentioned in the context.\n",
    "2. Based on step by step reasoning, select the appropriate answer(s) from the candidate answers, where DK means \"Don't Know\".\n",
    "Important: Pay special attention to the output format. The output should be 1-5 words. Do not include the reasoning process and explanation in the output answer.\n",
    "\"\"\"\n",
    "Resq_base_prompt = \"\"\"\n",
    "You are a helper bot who is especially skilled in spatial reasoning and other common sense reasoning tasks.\n",
    "Please answer Yes/No question with only Yes or No. Donot include the explanation and reasoning process in the output\"\"\"\n",
    "\n",
    "Resq_CoT_prompt = \"\"\"\n",
    "You are a helper bot who is especially skilled in spatial reasoning and other common sense reasoning tasks.\n",
    "When answering the Yes/No questions, try to think step by step.\n",
    "1. Carefully read the context and question.\n",
    "2. Identify the key objects and their attributes, and spatial relations mentioned in the question.\n",
    "3. Based on step by step reasoning, answer the question  with Yes or NO.\n",
    "Note: No reasoning process or explanation is needed.\n",
    "\"\"\"\n",
    "# Initialize language model\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "     \"\",\n",
    "    max_tokens=100,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# Define different prompt templates\n",
    "prompt_templates = {\n",
    "    \"base\": PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\", \"choices\"],\n",
    "        template=Resq_base_prompt,\n",
    "    ),\n",
    "    \"cot\": PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\", \"choices\"],\n",
    "        template=Resq_CoT_prompt,),\n",
    "}\n",
    "\n",
    "eval_llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    \"\",\n",
    "    max_tokens=100,\n",
    "    temperature=0.1\n",
    ")\n",
    "# Load evaluators\n",
    "exact_match_evaluator = load_evaluator(EvaluatorType.EXACT_MATCH)\n",
    "string_distance_evaluator = load_evaluator(EvaluatorType.STRING_DISTANCE)\n",
    "# criteria_evaluator = load_evaluator(\n",
    "#         EvaluatorType.CRITERIA,\n",
    "#         criteria={\n",
    "#             \"correctness\": \"Is the answer correct based on the given context and question?\"\n",
    "#         },\n",
    "#         llm=eval_llm  # Specify the model to use for criteria evaluation\n",
    "# )\n",
    "\n",
    "\n",
    "# Initialize the sentence transformer model\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def cosine_sim(text1, text2):\n",
    "    # Encode the texts to get their embeddings\n",
    "    embedding1 = sentence_model.encode([text1])\n",
    "    embedding2 = sentence_model.encode([text2])\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity = cosine_similarity(embedding1, embedding2)[0][0]\n",
    "    return similarity\n",
    "\n",
    "def evaluate_prompt(dataset, prompt_templates, llm):\n",
    "    results = {prompt_name: {\n",
    "        \"exact_match\": [], \n",
    "        \"string_distance\": [], \n",
    "        \"qa_score\": [],\n",
    "        \"semantic_similarity\": []\n",
    "    } for prompt_name in prompt_templates.keys()}\n",
    "    \n",
    "    # Load LangChain evaluators\n",
    "    exact_match_evaluator = load_evaluator(EvaluatorType.EXACT_MATCH)\n",
    "    string_distance_evaluator = load_evaluator(EvaluatorType.STRING_DISTANCE)\n",
    "    # qa_evaluator = QAEvalChain.from_llm(llm=llm)\n",
    "    \n",
    "    for prompt_name, prompt_template in prompt_templates.items():\n",
    "        chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "        \n",
    "        examples = []\n",
    "        \n",
    "        for row in dataset:\n",
    "            context = row[\"Story\"]\n",
    "            question = row[\"Question\"]\n",
    "            choices = row[\"Candidate_Answers\"]\n",
    "            ground_truth = row[\"Answer\"]\n",
    "            \n",
    "            response = chain.run(context=context, question=question, choices=choices)\n",
    "            \n",
    "            # Exact Match and String Distance\n",
    "            exact_match_result = exact_match_evaluator.evaluate_strings(prediction=response, reference=ground_truth)\n",
    "            string_distance_result = string_distance_evaluator.evaluate_strings(prediction=response, reference=ground_truth)\n",
    "            \n",
    "            # Custom Semantic Similarity\n",
    "            semantic_similarity_score = cosine_sim(response, ground_truth)\n",
    "            \n",
    "            results[prompt_name][\"exact_match\"].append(exact_match_result['score'])\n",
    "            results[prompt_name][\"string_distance\"].append(string_distance_result['score'])\n",
    "            results[prompt_name][\"semantic_similarity\"].append(semantic_similarity_score)\n",
    "            \n",
    "            # Prepare input for QA evaluator\n",
    "            examples.append({\n",
    "                \"query\": question,\n",
    "                \"answer\": ground_truth,\n",
    "                \"result\": response,\n",
    "                \"context\": context,\n",
    "            })\n",
    "        \n",
    "        # QA Evaluation\n",
    "        # # qa_results = qa_evaluator.evaluate(examples)\n",
    "        # for result in qa_results:\n",
    "        #     results[prompt_name][\"qa_score\"].append(result['score'])\n",
    "    \n",
    "    # Calculate average scores for each metric and prompt\n",
    "    final_results = {}\n",
    "    for prompt_name, prompt_results in results.items():\n",
    "        final_results[prompt_name] = {\n",
    "            metric: np.mean(scores) if scores else 0\n",
    "            for metric, scores in prompt_results.items()\n",
    "        }\n",
    "    \n",
    "    return final_results\n",
    "\n",
    "df = pd.read_csv('Resq.csv').sample(n=210, random_state=61)\n",
    "df = df.sample(frac=0.5, random_state=200)\n",
    "dataset = df.to_dict('records')\n",
    "results = evaluate_prompt(dataset, prompt_templates,llm)\n",
    "\n",
    "# Print results\n",
    "for prompt_name, metrics in results.items():\n",
    "    print(f\"Results for {prompt_name} prompt:\")\n",
    "    for metric, score in metrics.items():\n",
    "        print(f\"  {metric}: {score:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.evaluation import EvaluatorType\n",
    "from langchain.evaluation.loading import load_evaluator\n",
    "\n",
    "# Load  from environment variable (make sure to set this in your environment)\n",
    "\n",
    "Resq_base_prompt = \"\"\"\n",
    "You are a helper bot who is especially skilled in spatial reasoning and other common sense reasoning tasks.\n",
    "Please answer Yes/No question with only Yes or No. Donot include the explanation and reasoning process in the output\"\"\"\n",
    "\n",
    "Resq_CoT_prompt = \"\"\"\n",
    "You are a helper bot who is especially skilled in spatial reasoning and other common sense reasoning tasks.\n",
    "When answering the Yes/No questions, try to think step by step.\n",
    "1. Carefully read the context and question.\n",
    "2. Identify the key objects and their attributes, and spatial relations mentioned in the question.\n",
    "3. Based on step by step reasoning, answer the question  with Yes or NO.\n",
    "Note: No reasoning process or explanation is needed.\n",
    "\"\"\"\n",
    "# Initialize language model\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "     \"\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# Define different prompt templates\n",
    "prompt_templates = {\n",
    "    \"base\": PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\", \"choices\"],\n",
    "        template=Resq_base_prompt,\n",
    "    ),\n",
    "    \"cot\": PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\", \"choices\"],\n",
    "        template=Resq_CoT_prompt,),\n",
    "}\n",
    "\n",
    "def evaluate_and_save_responses(df, prompt_templates, llm, output_file):\n",
    "    \"\"\"\n",
    "    Evaluate prompts and save responses to a CSV file.\n",
    "    \n",
    "    :param df: DataFrame containing the dataset\n",
    "    :param prompt_templates: Dictionary of prompt templates\n",
    "    :param llm: Language model to use\n",
    "    :param output_file: Name of the output CSV file\n",
    "    :return: Updated DataFrame with model responses\n",
    "    \"\"\"\n",
    "    for prompt_name, prompt_template in prompt_templates.items():\n",
    "        chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "        \n",
    "        responses = []\n",
    "        for _, row in df.iterrows():\n",
    "            context = row[\"Story\"]\n",
    "            question = row[\"Question\"]\n",
    "            choices = row[\"Candidate_Answers\"]\n",
    "            \n",
    "            response = chain.run(context=context, question=question, choices=choices)\n",
    "            responses.append(response)\n",
    "        \n",
    "        # Add responses to the DataFrame\n",
    "        df[f'{prompt_name}_response'] = responses\n",
    "    \n",
    "    # Save the updated DataFrame to a CSV file\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Load and prepare data\n",
    "df = pd.read_csv('Resq.csv').sample(n=99, random_state=61)\n",
    "# Sample a subset if needed\n",
    "\n",
    "# Evaluate and save responses\n",
    "updated_df = evaluate_and_save_responses(df, prompt_templates, llm, 'model_responses.csv')\n",
    "\n",
    "print(\"Responses have been saved to 'model_responses.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def evaluate_and_save_responses(df, prompt_templates, llm, output_file):\n",
    "    \"\"\"\n",
    "    Evaluate prompts and save responses to a CSV file with error handling.\n",
    "    \n",
    "    :param df: DataFrame containing the dataset\n",
    "    :param prompt_templates: Dictionary of prompt templates\n",
    "    :param llm: Language model to use\n",
    "    :param output_file: Name of the output CSV file\n",
    "    :return: Updated DataFrame with model responses\n",
    "    \"\"\"\n",
    "    for prompt_name, prompt_template in prompt_templates.items():\n",
    "        chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "        \n",
    "        responses = []\n",
    "        for index, row in df.iterrows():\n",
    "            try:\n",
    "                context = row[\"Story\"]\n",
    "                question = row[\"Question\"]\n",
    "                choices = row[\"Candidate_Answers\"]\n",
    "                \n",
    "                full_prompt = prompt_template.format(context=context, question=question, choices=choices)\n",
    "                logger.info(f\"Full prompt for {prompt_name}, index {index}:\\n{full_prompt}\")\n",
    "                \n",
    "                response = chain.run(context=context, question=question, choices=choices)\n",
    "                logger.info(f\"Response for {prompt_name}, index {index}: {response}\")\n",
    "                \n",
    "                responses.append(response)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing row {index} for {prompt_name}: {str(e)}\")\n",
    "                responses.append(\"ERROR: \" + str(e))\n",
    "        \n",
    "        # Add responses to the DataFrame\n",
    "        df[f'{prompt_name}_response'] = responses\n",
    "    \n",
    "    # Save the updated DataFrame to a CSV file\n",
    "    df.to_csv(output_file, index=False)\n",
    "    logger.info(f\"Responses saved to {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "prompt_templates = {\n",
    "    \"base\": PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\", \"choices\"],\n",
    "        template=\"\"\"\n",
    "        Context: {context}\n",
    "        Question: {question}\n",
    "        Choices: {choices}\n",
    "        \n",
    "        Based on the context provided, please answer the question. Choose the best answer from the given choices. If you're not sure, you can answer 'DK' for \"Don't Know\".\n",
    "        \n",
    "        Your answer:\n",
    "        \"\"\"\n",
    "    ),\n",
    "    \"cot\": PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\", \"choices\"],\n",
    "        template=\"\"\"\n",
    "        Context: {context}\n",
    "        Question: {question}\n",
    "        Choices: {choices}\n",
    "        \n",
    "        Let's approach this step-by-step:\n",
    "        1) First, carefully read the context and question.\n",
    "        2) Consider each of the given choices.\n",
    "        3) Think through how each choice relates to the information in the context.\n",
    "        4) Select the best answer based on this analysis.\n",
    "        \n",
    "        If you're not sure after this process, you can answer 'DK' for \"Don't Know\".\n",
    "        \n",
    "        Your step-by-step reasoning and final answer:\n",
    "        \"\"\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('ReSQ.csv')\n",
    "\n",
    "# Sample a subset if needed (adjust as necessary)\n",
    "df_sample = df.sample(n=10, random_state=42)\n",
    "\n",
    "# Evaluate and save responses\n",
    "updated_df = evaluate_and_save_responses(df_sample, prompt_templates, llm, 'model_responses.csv')\n",
    "\n",
    "print(\"Responses have been saved to 'model_responses.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
