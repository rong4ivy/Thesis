{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364\n",
      "Q_type Dictionary {'YN': 143, 'CO': 79, 'FR': 77, 'FB': 65}\n",
      "140\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from typing import List, Union\n",
    "from fuzzywuzzy import fuzz\n",
    "import ast\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('Spar_test.csv')\n",
    "print(len(df))\n",
    "relations = df['Q_type'].unique()\n",
    "# Get the unique relations and their counts\n",
    "relation_counts = df['Q_type'].value_counts()\n",
    "# Create a dictionary of relations\n",
    "relation_dict = relation_counts.to_dict()\n",
    "# Print the relation dictionary\n",
    "print(\"Q_type Dictionary\",relation_dict)\n",
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('Spar.csv')\n",
    "\n",
    "\n",
    "# Convert string representations of lists to actual lists\n",
    "df['Answer'] = df['Answer'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Initialize an empty list to store the processed dataframes\n",
    "processed_dfs = []\n",
    "n_samples = 35\n",
    "\n",
    "# Iterate over each unique question type\n",
    "for question in df['Q_type'].unique():\n",
    "    # Filter the dataset for the current question type and length of Answer less than 2\n",
    "    filtered_df = df[(df['Q_type'] == question) & (df['Answer'].apply(lambda x: len(x) if isinstance(x, list) else 1) < 3)]\n",
    "    # Check if there are enough samples to sample from\n",
    "    if len(filtered_df) >= n_samples:\n",
    "        sampled_df = filtered_df.sample(n=n_samples, random_state=9999)\n",
    "        processed_dfs.append(sampled_df)\n",
    "    else:\n",
    "        print(f\"Not enough samples for Q_type {question}\")\n",
    "\n",
    "# Concatenate all processed dataframes\n",
    "result_df = pd.concat(processed_dfs, ignore_index=True)\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "print(len(result_df))\n",
    "result_df.to_csv('Spar_balanced_120_1.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_type Dictionary {'FB': 38, 'YN': 32, 'CO': 31, 'FR': 29}\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset\n",
    "df = pd.read_csv('spar_300_test.csv')\n",
    "df1= df.head(130)\n",
    "\n",
    "# Get the unique relations and their counts\n",
    "relation_counts = df1['Q_type'].value_counts()\n",
    "# Create a dictionary of relations\n",
    "relation_dict = relation_counts.to_dict()\n",
    "# Print the relation dictionary\n",
    "print(\"Q_type Dictionary\",relation_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_type\n",
      "FR    80\n",
      "CO    79\n",
      "YN    76\n",
      "FB    57\n",
      "Name: count, dtype: int64\n",
      "Combined CSV file created with 292 rows.\n",
      "Q_type\n",
      "FR    80\n",
      "CO    79\n",
      "YN    76\n",
      "FB    57\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the two CSV files\n",
    "df2= pd.read_csv('5_ollama_models_Base.csv')\n",
    "df1 = pd.read_csv('5_ollama_models_Base_full.csv')\n",
    "\n",
    "\n",
    "# Concatenate the dataframes based on the 'Question' column\n",
    "result = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
    "\n",
    "# Remove duplicate rows based on the 'Question' column, keeping the first occurrence\n",
    "result = result.drop_duplicates(subset='Question', keep='first')\n",
    "\n",
    "# Reset the index of the resulting dataframe\n",
    "result = result.reset_index(drop=True)\n",
    "relation_counts = result ['Q_type'].value_counts()\n",
    "print(relation_counts)\n",
    "# Save the result to a new CSV file\n",
    "result.to_csv('spar_combined_1.csv', index=False)\n",
    "\n",
    "print(f\"Combined CSV file created with {len(result)} rows.\")\n",
    "\n",
    "df = pd.read_csv('spar_combined_1.csv')\n",
    "counts = df['Q_type'].value_counts()\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('spar_300_test.csv')\n",
    "df = df.sample(frac=1, random_state=42)\n",
    "df.to_csv('spar_300_test_balanced.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from typing import List, Union\n",
    "from fuzzywuzzy import fuzz\n",
    "import ast\n",
    "\n",
    "df = pd.read_csv('Spar.csv')\n",
    "print(len(df))\n",
    "relations = df['target_relation'].unique()\n",
    "# Get the unique relations and their counts\n",
    "relation_counts = df['target_relation'].value_counts()\n",
    "# Create a dictionary of relations\n",
    "relation_dict = relation_counts.to_dict()\n",
    "# Print the relation dictionary\n",
    "print(\"Relation Dictionary\",relation_dict)\n",
    "relation_distribution = df['target_relation'].value_counts() \n",
    "\n",
    "# Define custom colors for the bars\n",
    "custom_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "relation_distribution.plot(kind='bar', color=custom_colors)\n",
    "plt.title('Distribution of target_relation types in Spartun', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Relation Types', fontsize=12)\n",
    "plt.ylabel('Counts', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=11)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)  # Add grid lines\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('relation_distribution_plot.png', dpi=300)  # Save the plot with higher resolution\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics for CoT prompt                   accuracy\n",
      "ollama/phi3:mini  0.328000\n",
      "ollama/gemma:2b   0.002667\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(true_answers, pred_answers):\n",
    "    correct = 0\n",
    "    total = len(true_answers)\n",
    "\n",
    "    def extract_answer(text):\n",
    "        if isinstance(text, str):\n",
    "            match = re.search(r'(answer|the answer is)\\s*:\\s*([a-zA-Z]+)', text, re.IGNORECASE)\n",
    "            if match:\n",
    "                return match.group(2).strip()\n",
    "        return text  # Return the original text if no match is found or if it's not a string\n",
    "\n",
    "    def normalize_answer(answer):\n",
    "        if isinstance(answer, str):\n",
    "            # First, try to extract the answer if it's in the format \"answer: ...\"\n",
    "            answer = extract_answer(answer)\n",
    "            # Then, try to evaluate if it's a string representation of a list\n",
    "            try:\n",
    "                lst = ast.literal_eval(answer)\n",
    "               # Join list elements into a single string\n",
    "                return  [','.join(str(item) for item in lst)]\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if isinstance(answer, list):\n",
    "            return [str(item).lower().strip() for item in answer]\n",
    "        else:\n",
    "            return [str(answer).lower().strip()]\n",
    "\n",
    "    for true, pred in zip(true_answers, pred_answers):\n",
    "        if pd.isna(true) or pd.isna(pred):\n",
    "            total -= 1  # Skip this pair if either is NaN\n",
    "            continue\n",
    "\n",
    "        true_set = set(normalize_answer(true))\n",
    "        pred_set = set(normalize_answer(pred))\n",
    "\n",
    "        if len(true_set) > 1:  # Multiple correct answers\n",
    "            if true_set == pred_set:\n",
    "                correct += 1  # Full match\n",
    "            elif true_set.intersection(pred_set):\n",
    "                correct += 0.4  # Partial match\n",
    "        else:  # Single correct answer\n",
    "            if true_set == pred_set:\n",
    "                correct += 1\n",
    "\n",
    "    return correct / total if total > 0 else 0\n",
    "\n",
    "df_results = pd.read_csv('2_small_Base.csv')\n",
    "models = [\"ollama/phi3:mini\",\"ollama/gemma:2b\"]\n",
    "metrics = {}\n",
    "for model in models:\n",
    "    accuracy = calculate_accuracy(df_results['Answer'], df_results[model])\n",
    "    metrics[model] = {'accuracy': accuracy}\n",
    "# Create a DataFrame with the metrics\n",
    "metrics_df = pd.DataFrame(metrics).T\n",
    "print(\"metrics for CoT prompt\", metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'e', 'l', 'a', 'p'}\n"
     ]
    }
   ],
   "source": [
    "def normalize_answer(answer):\n",
    "        if isinstance(answer, str):\n",
    "            # First, try to extract the answer if it's in the format \"answer: ...\"\n",
    "    \n",
    "            # Then, try to evaluate if it's a string representation of a list\n",
    "            try:\n",
    "                lst = ast.literal_eval(answer)\n",
    "               # Join list elements into a single string\n",
    "                return  [','.join(str(item) for item in lst)]\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if isinstance(answer, list):\n",
    "            return [str(item).lower().strip() for item in answer]\n",
    "        else:\n",
    "            return [str(answer).lower().strip()]\n",
    "re = set('apple')\n",
    "print(re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/autogen/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def calculate_accuracy_transformer(true_answers: pd.Series, pred_answers: pd.Series) -> float:\n",
    "    # Load a pre-trained sentence transformer model\n",
    "    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "    def normalize(answer):\n",
    "        if pd.isna(answer):\n",
    "            return \"\"\n",
    "        # Try to evaluate as a literal Python expression (for list-like strings)\n",
    "        try:\n",
    "            answer_list = ast.literal_eval(answer)\n",
    "            if isinstance(answer_list, list):\n",
    "                return \", \".join(str(item).strip() for item in answer_list)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # If not a list, return as is\n",
    "        return str(answer).strip()\n",
    "\n",
    "    def compute_similarity(true, pred):\n",
    "        # Encode sentences\n",
    "        true_embedding = model.encode([true])\n",
    "        pred_embedding = model.encode([pred])\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        similarity = cosine_similarity(true_embedding, pred_embedding)[0][0]\n",
    "        \n",
    "        # Adjust similarity score to consider partial matches\n",
    "        if similarity > 0.9:  # High similarity, consider as full match\n",
    "            return 1.0\n",
    "        elif similarity > 0.7:  # Moderate similarity, consider as partial match\n",
    "            return 0.5\n",
    "        else:\n",
    "            return similarity  # Low similarity, return as is\n",
    "\n",
    "    total_similarity = 0\n",
    "    valid_comparisons = 0\n",
    "\n",
    "    for true, pred in zip(true_answers, pred_answers):\n",
    "        true_normalized = normalize(true)\n",
    "        pred_normalized = normalize(pred)\n",
    "\n",
    "        if not true_normalized or not pred_normalized:\n",
    "            continue\n",
    "\n",
    "        similarity = compute_similarity(true_normalized, pred_normalized)\n",
    "        total_similarity += similarity\n",
    "        valid_comparisons += 1\n",
    "\n",
    "    accuracy = total_similarity / valid_comparisons if valid_comparisons > 0 else 0.0\n",
    "\n",
    "    return accuracy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
