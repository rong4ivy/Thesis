{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ollama/gemma\n",
      " Mean Accuracy: 0.3708791208791209\n",
      "Model: ollama/openchat\n",
      " Mean Accuracy: 0.44329896907216493\n",
      "Model: ollama/mistral\n",
      " Mean Accuracy: 0.38144329896907214\n",
      "Model: ollama/llama3\n",
      " Mean Accuracy: 0.45017182130584193\n",
      "Model: ollama/gemma:2b\n",
      " Mean Accuracy: 0.29553264604810997\n",
      "Model: ollama/phi3:mini\n",
      " Mean Accuracy: 0.3024054982817869\n",
      "Model: ollama/qwen:4b\n",
      " Mean Accuracy: 0.41580756013745707\n",
      "Model: llama70B\n",
      " Mean Accuracy: 0.5738831615120275\n",
      "Accuracy by Q_type for each model:\n",
      "\n",
      "ollama/gemma:2b:\n",
      "  CO: 0.2206\n",
      "  FB: 0.3514\n",
      "  FR: 0.0635\n",
      "  YN: 0.4767\n",
      "\n",
      "ollama/phi3:mini:\n",
      "  CO: 0.1029\n",
      "  FB: 0.3108\n",
      "  FR: 0.1587\n",
      "  YN: 0.5581\n",
      "\n",
      "ollama/qwen:4b:\n",
      "  CO: 0.4118\n",
      "  FB: 0.4730\n",
      "  FR: 0.2222\n",
      "  YN: 0.5116\n",
      "\n",
      "ollama/openchat:\n",
      "  CO: 0.3971\n",
      "  FB: 0.5541\n",
      "  FR: 0.0952\n",
      "  YN: 0.6395\n",
      "\n",
      "ollama/mistral:\n",
      "  CO: 0.3824\n",
      "  FB: 0.4730\n",
      "  FR: 0.0476\n",
      "  YN: 0.5465\n",
      "\n",
      "ollama/llama3:\n",
      "  CO: 0.3382\n",
      "  FB: 0.5135\n",
      "  FR: 0.2222\n",
      "  YN: 0.6512\n",
      "\n",
      "llama70B:\n",
      "  CO: 0.5000\n",
      "  FB: 0.7568\n",
      "  FR: 0.2698\n",
      "  YN: 0.6977\n",
      "\n",
      "Overall metrics:\n",
      "ollama/gemma: 0.3709\n",
      "ollama/openchat: 0.4433\n",
      "ollama/mistral: 0.3814\n",
      "ollama/llama3: 0.4502\n",
      "ollama/gemma:2b: 0.2955\n",
      "ollama/phi3:mini: 0.3024\n",
      "ollama/qwen:4b: 0.4158\n",
      "llama70B: 0.5739\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 104\u001b[0m\n\u001b[1;32m    101\u001b[0m melted_df \u001b[38;5;241m=\u001b[39m accuracy_by_qtype\u001b[38;5;241m.\u001b[39mmelt(id_vars\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQ_type\u001b[39m\u001b[38;5;124m'\u001b[39m, var_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m, value_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Plot the data\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m    105\u001b[0m sns\u001b[38;5;241m.\u001b[39mbarplot(data\u001b[38;5;241m=\u001b[39mmelted_df, x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQ_type\u001b[39m\u001b[38;5;124m'\u001b[39m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, hue\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    106\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBase Prompt: Model Accuracy by Question Type\u001b[39m\u001b[38;5;124m'\u001b[39m, fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from typing import List, Union\n",
    "import ast\n",
    "\n",
    "\n",
    "def calculate_accuracy(true_answers, pred_answers):\n",
    "    correct = 0\n",
    "    total = len(true_answers)\n",
    "    def extract_answer(text):\n",
    "        \n",
    "        # Match \"the answer is\" or \"answer\" followed by any text\n",
    "        match = re.search(r\"(?:the answer is|answer)(?:\\s*:?\\s*)(.+)\", text, re.IGNORECASE) #([^.]+)(?:\\.|$)\"\n",
    "        if match:\n",
    "                text = match.group(1).strip()\n",
    "                # Check if the text begins with \"yes\" or \"no\" and is longer than one word\n",
    "        yes_no_match = re.match(r'^(yes|no)\\b', text, re.IGNORECASE)\n",
    "        if yes_no_match:\n",
    "                text= yes_no_match.group(1).lower()\n",
    "        return text\n",
    "    \n",
    "\n",
    "    def normalize_answer(answer):\n",
    "        answer = extract_answer(answer)\n",
    "        if isinstance(answer, str):\n",
    "        # Then, try to evaluate if it's a string representation of a list\n",
    "            try:\n",
    "                lst = ast.literal_eval(answer)\n",
    "            # Join list elements into a single string, remove special characters, and strip\n",
    "                return [re.sub(r'[^\\w\\s]', ' ', ','.join(str(item) for item in lst)).strip().lower()]\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        if isinstance(answer, list):\n",
    "           return [re.sub(r'[^\\w\\s]', ' ', str(item)).strip().lower() for item in answer]\n",
    "        else:\n",
    "           return [re.sub(r'[^\\w\\s]', ' ', str(answer)).strip().lower()]\n",
    "    \n",
    "    for true, pred in zip(true_answers, pred_answers):\n",
    "        if pd.isna(true) or pd.isna(pred):\n",
    "            total -= 1  # Skip this pair if either is NaN\n",
    "            continue\n",
    "\n",
    "        true_set = set(normalize_answer(true))\n",
    "        pred_set = set(normalize_answer(pred))\n",
    "\n",
    "        if len(true_set) > 1:  # Multiple correct answers\n",
    "            if true_set == pred_set:\n",
    "                correct += 1  # Full match\n",
    "            # elif true_set.intersection(pred_set):\n",
    "            #     correct += 0.5  # Partial match\n",
    "        else:  # Single correct answer\n",
    "            if true_set == pred_set:\n",
    "                correct += 1\n",
    "\n",
    "    return correct / total if total > 0 else 0\n",
    "try:\n",
    "    df_results = pd.read_csv('7_models_Base_prompt_300.csv', encoding='utf-8')\n",
    "except UnicodeDecodeError:\n",
    "    try:\n",
    "        df_results = pd.read_csv('7_models_Base_prompt_300.csv', encoding='latin1')\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            df_results = pd.read_csv('7_models_Base_prompt_300.csv', encoding='cp1252')\n",
    "        except UnicodeDecodeError as e:\n",
    "            print(f\"Unable to decode the file with utf-8, latin1, or cp1252 encodings: {e}\")\n",
    "            raise\n",
    "models = [\"ollama/gemma:2b\",\"ollama/phi3:mini\",\"ollama/qwen:4b\",\"ollama/openchat\", \"ollama/mistral\", \"ollama/llama3\",\"llama70B\"]\n",
    "# models = [\"ollama/gemma\",\"ollama/openchat\", \"ollama/mistral\", \"ollama/llama3\"]\n",
    "# metrics = {}\n",
    "# df_results = pd.read_csv('Base.csv')\n",
    "\n",
    "for model in models:\n",
    "    df_results[f'{model}_accuracy'] = df_results.apply(lambda row: calculate_accuracy([row['Answer']], [row[model]]), axis=1)\n",
    "    metrics[model] = df_results[f'{model}_accuracy'].mean()\n",
   
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Data for overall metrics and model sizes\n",
    "data = {\n",
    "    'Model': ['Gemma 2B', 'Phi3: mini 3B', 'Qwen:4B', 'Openchat 8B', 'Mistral 8B', 'Llama3 8B', 'Llama3 70B', 'Deepseek chat 200B'],\n",
    "    'Size (B)': [2, 3, 4, 8, 8, 8, 70, 200],\n",
    "   'Accuracy': [16.55, 30.24, 40.38, 42.35, 38.14, 42.74, 52.37, 58.64]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create the line plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=df, x='Size (B)', y='Accuracy', marker='o')\n",
    "\n",
    "# Annotate each point with the model name\n",
    "for i, row in df.iterrows():\n",
    "    plt.annotate(row['Model'], (row['Size (B)'], row['Accuracy']), \n",
    "                 xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "plt.title('Model Size vs Overall Accuracy with Zero-shot Prompt', fontsize=16)\n",
    "plt.xlabel('Model Size (Billion Parameters)', fontsize=12)\n",
    "plt.ylabel('Overall Accuracy', fontsize=12)\n",
    "plt.xscale('log')  # Use log scale for x-axis due to large range\n",
    "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Data for question type accuracies\n",
    "data_qtype = {\n",
    "    'Model': ['Gemma 2B', 'Phi3: mini 3B', 'Qwen:4B', 'Openchat 8B', 'Mistral 8B', 'Llama3 8B', 'Llama3 70B', 'Deepseek 200B'],\n",
    "\"CO\": [0.0645, 0.0871, 0.4124, 0.3983, 0.3454, 0.3063, 0.4983, 0.4799],\n",
    "\"FB\": [0.0806, 0.2454, 0.4727, 0.5479, 0.427, 0.4594, 0.7577, 0.68],\n",
    "\"FR\": [0.0322, 0.1267, 0.2213, 0.0996, 0.0454, 0.1982, 0.2692, 0.342],\n",
    "\"YN\": [0.1558, 0.4592, 0.5431, 0.7072, 0.6359, 0.7657, 0.7875, 0.7399],\n",
    "}\n",
    "\n",
    "\n",
    "df_qtype = pd.DataFrame(data_qtype)\n",
    "\n",
    "# Melt the DataFrame\n",
    "melted_df = pd.melt(df_qtype, id_vars=['Model'], var_name='Q_type', value_name='Accuracy')\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.barplot(data=melted_df, x='Q_type', y='Accuracy', hue='Model')\n",
    "plt.title('Zero-Shot Prompt: Model Accuracy by Question Type', fontsize=16)\n",
    "plt.xlabel('Question Type', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy by Q_type for each model:\n",
    "\n",
    "ollama/gemma:2b:\n",
    "  CO: 0.2206\n",
    "  FB: 0.3514\n",
    "  FR: 0.0635\n",
    "  YN: 0.4767\n",
    "\n",
    "ollama/phi3:mini:\n",
    "  CO: 0.1029\n",
    "  FB: 0.3108\n",
    "  FR: 0.1587\n",
    "  YN: 0.5581\n",
    "\n",
    "ollama/qwen:4b:\n",
    "  CO: 0.4118\n",
    "  FB: 0.4730\n",
    "  FR: 0.2222\n",
    "  YN: 0.5116\n",
    "\n",
    "ollama/openchat:\n",
    "  CO: 0.3971\n",
    "  FB: 0.5541\n",
    "  FR: 0.0952\n",
    "  YN: 0.6395\n",
    "\n",
    "ollama/mistral:\n",
    "  CO: 0.3824\n",
    "  FB: 0.4730\n",
    "  FR: 0.0476\n",
    "  YN: 0.5465\n",
    "\n",
    "ollama/llama3:\n",
    "  CO: 0.3382\n",
    "  FB: 0.5135\n",
    "  FR: 0.2222\n",
    "  YN: 0.6512\n",
    "\n",
    "llama70B:\n",
    "  CO: 0.5000\n",
    "  FB: 0.7568\n",
    "  FR: 0.2698\n",
    "  YN: 0.6977\n",
    "\n",
    "Overall metrics:Base\n",
    "ollama/gemma:2b: 0.2955\n",
    "ollama/phi3:mini: 0.3024\n",
    "ollama/qwen:4b: 0.4158\n",
    "ollama/openchat: 0.4433\n",
    "ollama/mistral: 0.3814\n",
    "ollama/llama3: 0.4502\n",
    "llama70B: 0.5739\n",
    "\n",
    "Overall metrics: Visual\n",
    "ollama/gemma:2b: 0.0344\n",
    "ollama/phi3:mini: 0.3643\n",
    "ollama/qwen:4b: 0.3093\n",
    "ollama/openchat: 0.4192\n",
    "ollama/mistral: 0.3814\n",
    "ollama/llama3: 0.4330\n",
    "llama70B: 0.5739\n",
    "\n",
    "Model: ollama/gemma:2b\n",
    " Mean Accuracy: 0.03436426116838488\n",
    "Model: ollama/phi3:mini\n",
    " Mean Accuracy: 0.3642611683848797\n",
    "Model: ollama/qwen:4b\n",
    " Mean Accuracy: 0.30927835051546393\n",
    "Model: ollama/openchat\n",
    " Mean Accuracy: 0.41924398625429554\n",
    "Model: ollama/mistral\n",
    " Mean Accuracy: 0.38144329896907214\n",
    "Model: ollama/llama3\n",
    " Mean Accuracy: 0.4329896907216495\n",
    "Model: llama70B\n",
    " Mean Accuracy: 0.5738831615120275\n",
    "Accuracy by Q_type for each model:\n",
    "\n",
    "ollama/gemma:2b:\n",
    "  CO: 0.0147\n",
    "  FB: 0.0000\n",
    "  FR: 0.0000\n",
    "  YN: 0.1047\n",
    "\n",
    "ollama/phi3:mini:\n",
    "  CO: 0.2353\n",
    "  FB: 0.5135\n",
    "  FR: 0.1270\n",
    "  YN: 0.5116\n",
    "\n",
    "ollama/qwen:4b:\n",
    "  CO: 0.0588\n",
    "  FB: 0.4189\n",
    "  FR: 0.0952\n",
    "  YN: 0.5698\n",
    "\n",
    "ollama/openchat:\n",
    "  CO: 0.4118\n",
    "  FB: 0.5811\n",
    "  FR: 0.0952\n",
    "  YN: 0.5233\n",
    "\n",
    "ollama/mistral:\n",
    "  CO: 0.3382\n",
    "  FB: 0.5405\n",
    "  FR: 0.0476\n",
    "  YN: 0.5233\n",
    "\n",
    "ollama/llama3:\n",
    "  CO: 0.4706\n",
    "  FB: 0.5000\n",
    "  FR: 0.1111\n",
    "  YN: 0.5814\n",
    "\n",
    "llama70B:\n",
    "  CO: 0.4853\n",
    "  FB: 0.7568\n",
    "  FR: 0.2857\n",
    "  YN: 0.6977\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
